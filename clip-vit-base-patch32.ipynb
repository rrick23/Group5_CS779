{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9977081,"sourceType":"datasetVersion","datasetId":6128881}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Use GPU for faster results","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom PIL import Image\nfrom tqdm import tqdm\nimport pandas as pd\nimport os\n\ndf2 = pd.read_csv('/kaggle/input/semeval/SemEval2025/test.tsv', sep=\"\\t\")\nipath2 = []\nsentences2 = []\n\nfor i, row in df2.iterrows():\n    compound = row['compound']\n    expected_order = row['expected_order']\n    ch = \"\".join(char for char in compound if char != \" \")\n    base_path = f'/kaggle/input/semeval/SemEval2025/train/{ch}'\n\n    if os.path.exists(base_path):\n        expected_order = expected_order.strip(\"[]\").replace(\"'\", \"\").split(\",\") \n\n        for img_filename in expected_order:\n            img_filename = img_filename.strip()\n            full_path = os.path.join(base_path, img_filename)\n            if os.path.isfile(full_path):\n                ipath2.append(full_path)\n            else:\n                print(f\"File not found: {full_path}\")\n    else:\n        print(f\"Directory not found: {base_path}\")\n    sentences2.append(row['sentence'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T20:54:10.097563Z","iopub.execute_input":"2024-11-22T20:54:10.098006Z","iopub.status.idle":"2024-11-22T20:54:13.637855Z","shell.execute_reply.started":"2024-11-22T20:54:10.097955Z","shell.execute_reply":"2024-11-22T20:54:13.636905Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import torch\nfrom tqdm import tqdm\nfrom transformers import CLIPProcessor, CLIPModel\nfrom PIL import Image\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nnew_clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\nnew_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\ndef calculate_topk_accuracy(k):\n    new_clip_model.eval()\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for i, sentence in enumerate(tqdm(sentences2, desc=f\"Evaluating Top-{k} Accuracy on Test Data\")):\n            text_inputs = new_processor(text=sentence, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n            text_features = new_clip_model.get_text_features(\n                input_ids=text_inputs[\"input_ids\"], \n                attention_mask=text_inputs[\"attention_mask\"]\n            )\n            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n            image_start_idx = 5 * i\n            image_end_idx = image_start_idx + 4\n            selected_image_paths = ipath2[image_start_idx:image_end_idx + 1]\n            image_embeddings_list = []\n            \n            for path in selected_image_paths:\n                image = Image.open(path).convert(\"RGB\")  # Ensure image is in RGB mode\n                pixel_values = new_processor(images=image, return_tensors=\"pt\").pixel_values.to(device)\n                image_features = new_clip_model.get_image_features(pixel_values=pixel_values)\n                image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n                image_embeddings_list.append(image_features)\n\n            image_embeddings = torch.cat(image_embeddings_list, dim=0)\n            similarities = torch.matmul(text_features, image_embeddings.T)\n            similarities = similarities.squeeze(0)\n            ranked_indices = torch.argsort(similarities, descending=True)\n            if 0 in ranked_indices[:k]:\n                correct += 1\n\n            total += 1\n\n    topk_accuracy = correct / total * 100\n    return topk_accuracy\n\ntop1_accuracy_new = calculate_topk_accuracy(1)\nprint(f\"Top-1 Accuracy on Test Data (New Model): {top1_accuracy_new:.4f}%\")\ntop2_accuracy_new = calculate_topk_accuracy(2)\nprint(f\"Top-2 Accuracy on Test Data (New Model): {top2_accuracy_new:.4f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T20:54:20.511890Z","iopub.execute_input":"2024-11-22T20:54:20.512241Z","iopub.status.idle":"2024-11-22T20:54:48.995794Z","shell.execute_reply.started":"2024-11-22T20:54:20.512211Z","shell.execute_reply":"2024-11-22T20:54:48.994821Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.19k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af80c518aee6410296f39433233b4867"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1395bec196c43ae91fd32f18e192a2d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c7b09b728d54e69a6a296d13598da01"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"673511d39e8a4813b968710672b79a0a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/862k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8510ef2172ef4a1cadb1c93a33ffc386"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d5b225573bd432eab2d8ad9356f41c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b311f771597e4a2bb3f1dcef9051c122"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48c15de9c693410394100a0d036d5f44"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nEvaluating Top-1 Accuracy on Test Data: 100%|██████████| 15/15 [00:05<00:00,  2.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Top-1 Accuracy on Test Data (New Model): 53.3333%\n","output_type":"stream"},{"name":"stderr","text":"Evaluating Top-2 Accuracy on Test Data: 100%|██████████| 15/15 [00:03<00:00,  4.32it/s]","output_type":"stream"},{"name":"stdout","text":"Top-2 Accuracy on Test Data (New Model): 66.6667%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":2}]}